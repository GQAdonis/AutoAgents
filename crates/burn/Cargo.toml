[package]
name = "autoagents-burn"
version.workspace = true
edition.workspace = true
license.workspace = true
description.workspace = true
repository.workspace = true
keywords.workspace = true
categories.workspace = true

[lib]
crate-type = ["cdylib", "rlib"]

[features]
default = ["tiny"]
ndarray = []
cuda = ["burn/cuda", "burn/default"]

pretrained = ["burn/network", "dep:dirs"]
tiny = []
llama3 = ["tiktoken-rs"]


test-non-default = []
test-ndarray = ["burn/ndarray"]

[dependencies]
autoagents-llm.workspace = true
burn = { git = "https://github.com/tracel-ai/burn", features = [
    "std", "ndarray", "webgpu"
], default-features = false, rev = "c56f5ab2046c7dbc28e642c0a08624c4ec290106" }
burn-common = { git = "https://github.com/tracel-ai/burn", features = [
    "std",
], default-features = false, rev = "c56f5ab2046c7dbc28e642c0a08624c4ec290106" }
burn-import = { git = "https://github.com/tracel-ai/burn", default-features = false, rev = "c56f5ab2046c7dbc28e642c0a08624c4ec290106" }


serde = { version = "1.0.192", default-features = false, features = [
    "derive",
    "alloc",
] } # alloc is for no_std, derive is needed

dirs = { workspace = true, optional = true }


# Tiktoken tokenizer (llama 3)
tiktoken-rs = { version = "0.5", optional = true }
base64 = { version = "0.22" }
rustc-hash = { version = "1.1" }

# SentencePiece tokenizer (tiny llama / llama 2)
tokenizers = { version = "0.22.0", default-features = false, features = [
    "unstable_wasm",
] }

rand = { workspace = true }
futures = { workspace = true }
cfg-if = "1.0.3"

[target.'cfg(not(target_arch = "wasm32"))'.dependencies]
tokio-stream.workspace = true
tokio = { workspace = true }


[target.'cfg(target_arch = "wasm32")'.dependencies]
wasm-bindgen = "0.2"
wasm-bindgen-futures = "0.4.50"
getrandom = { workspace = true, features = ["wasm_js"] }
uuid = { workspace = true, features = ["serde", "v4", "js"] }

#burn-lm-llama = { git = "https://github.com/tracel-ai/burn-lm", default-features = false, features = [] }